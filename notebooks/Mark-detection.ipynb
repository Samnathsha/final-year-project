{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAckSQgGaGHY"
      },
      "source": [
        "### Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mkW9MWLe9D4",
        "outputId": "05377f96-38a2-40a8-a134-31fc39a65628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository: 'deb https://ppa.launchpadcontent.net/alex-p/tesseract-ocr-devel/ubuntu/ jammy main'\n",
            "Description:\n",
            "This PPA contains an OCR engine - libtesseract and a command line program - tesseract. The development version available here (currntly 5.0.0 ) is better in many aspects (functionality, speed, stability) but is not 100 % API compatible with version 4.0. Tesseract 4 added a new neural net (LSTM) based OCR engine which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0). It also needs traineddata files which support the legacy engine, for example those from the tessdata repository. Tesseract has unicode (UTF-8) support, and can recognize more than 100 languages \"out of the box\". Tesseract supports various output formats: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV. The master branch also has experimental support for ALTO (XML) output.\n",
            "More info: https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr-devel\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/alex-p-ubuntu-tesseract-ocr-devel-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/alex-p-ubuntu-tesseract-ocr-devel-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/alex-p-ubuntu-tesseract-ocr-devel.gpg with fingerprint 8529B1E0F8BF7F65C12FABB0A4BCBD87CEF9E52D\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [812 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/alex-p/tesseract-ocr-devel/ubuntu jammy InRelease [17.6 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [23.8 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/alex-p/tesseract-ocr-devel/ubuntu jammy/main amd64 Packages [25.0 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,268 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]\n",
            "62% [18 Packages store 0 B] [19 Packages 123 kB/1,369 kB 9%] [16 Sources 187 kB/2,268 kB 8%]"
          ]
        }
      ],
      "source": [
        "!add-apt-repository -y ppa:alex-p/tesseract-ocr-devel\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "outputs": [],
      "source": [
        "# !dpkg --configure -a\n",
        "# !export PATH=/usr/local/lib/python3.10/dist-packages/pytesseract:$PATH\n",
        "!wget \"https://onedrive.live.com/download?cid=DBD24D3DC9712F7A&resid=DBD24D3DC9712F7A%2110213&authkey=AKAJrobqemGhan8\" -O multi_column_data.zip\n",
        "!unzip -qq multi_column_data.zip\n",
        "%cd multi_column_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhITY1QjkmdO"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/doctorai-in/upload-images/raw/main/atevH.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "!pip install img2table\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from matplotlib import pyplot as plt\n",
        "from pytesseract import Output\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from imutils import contours\n",
        "import img2table\n",
        "from img2table.ocr import TesseractOCR\n",
        "from img2table.document import Image\n",
        "from PIL import Image as im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRiCD7sKjnYY"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heHo3Fp7jm04"
      },
      "outputs": [],
      "source": [
        "def plt_imshow(title, image):\n",
        "\t# convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPsafVRcnlFP"
      },
      "outputs": [],
      "source": [
        "def find_red_contours(image):\n",
        "  \"\"\"\n",
        "  Finds contours of red-colored objects in the image.\n",
        "\n",
        "  Args:\n",
        "      image: The input image (BGR format).\n",
        "\n",
        "  Returns:\n",
        "      A list of contours representing red objects in the image.\n",
        "  \"\"\"\n",
        "  # Convert BGR to HSV color space\n",
        "  hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  # Define lower and upper bounds for red color in HSV\n",
        "  # lower boundary RED color range values; Hue (0 - 10)\n",
        "  lower1 = np.array([0, 20, 120])\n",
        "  upper1 = np.array([5, 130, 220])\n",
        "\n",
        "  # upper boundary RED color range values; Hue (160 - 180)\n",
        "  lower2 = np.array([335,20,120])\n",
        "  upper2 = np.array([360,130,220])\n",
        "\n",
        "  lower_mask = cv2.inRange(hsv, lower1, upper1)\n",
        "  upper_mask = cv2.inRange(hsv, lower2, upper2)\n",
        "\n",
        "  mask = cv2.bitwise_or(lower_mask, upper_mask)\n",
        "\n",
        "  # Find contours in the mask\n",
        "  contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  # Return the list of red contours\n",
        "  return contours\n",
        "\n",
        "def cut_out_red_numbers(image):\n",
        "  \"\"\"\n",
        "  Cuts out the regions containing red contours (assumed to be numbers).\n",
        "\n",
        "  Args:\n",
        "      image: The input image (BGR format).\n",
        "\n",
        "  Returns:\n",
        "      A list of images containing the regions with red contours.\n",
        "  \"\"\"\n",
        "  # Find red contours\n",
        "  red_contours = find_red_contours(image)\n",
        "\n",
        "  # Initialize list to store cut-out regions\n",
        "  cut_images = []\n",
        "\n",
        "  # Loop through each red contour and extract the corresponding region\n",
        "  for cnt in red_contours:\n",
        "    x, y, w, h = cv2.boundingRect(cnt)\n",
        "    cut_image = image[y:y+h, x:x+w]\n",
        "    cut_images.append(cut_image)\n",
        "\n",
        "  return cut_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emXaJVNbgNOX"
      },
      "outputs": [],
      "source": [
        "def get_dominant_color(image):\n",
        "    \"\"\"Calculates the dominant color of an image using K-Means clustering.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the dominant color (B, G, R) or None if clustering fails.\n",
        "    \"\"\"\n",
        "    image_reshaped = image.reshape((image.shape[0] * image.shape[1], image.shape[2])).astype(np.float32)\n",
        "    num_clusters = 5  # Adjust this parameter if needed\n",
        "\n",
        "    # Create a TermCriteria object for K-Means termination\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "\n",
        "    try:\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = cv2.kmeans(image_reshaped, num_clusters, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "        dominant_color = kmeans[1][0]  # Access the first centroid (dominant color)\n",
        "        return dominant_color.astype(np.uint8)  # Convert to int\n",
        "    except:\n",
        "        print(\"K-Means clustering failed for this image.\")\n",
        "        return None  # Return None if clustering fails\n",
        "\n",
        "def find_most_red_image(images):\n",
        "    most_red_image = None\n",
        "    highest_red_score = 0.0\n",
        "\n",
        "    for image in images:\n",
        "        try:\n",
        "            dominant_color = get_dominant_color(image)\n",
        "            if dominant_color is None:  # Skip if K-Means fails\n",
        "                continue\n",
        "\n",
        "            if len(dominant_color) == 3:  # Ensure dominant_color has expected length\n",
        "                red_score = dominant_color[2] * 0.8 + count * 0.2  # Use K-Means color\n",
        "            else:\n",
        "                red_score = np.mean(image[:, :, 2])  # Fallback to image average\n",
        "\n",
        "            if red_score > highest_red_score:\n",
        "                highest_red_score = red_score\n",
        "                most_red_image = image\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {e}\")\n",
        "\n",
        "    return most_red_image\n",
        "\n",
        "def preprocess_digit_image(image):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply binary thresholding to obtain a binary image\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours in the binary image\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    digit_images = []\n",
        "    for cnt in contours:\n",
        "        # Get the bounding box of the contour\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "\n",
        "        # Crop the digit from the original image\n",
        "        digit_roi = image[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize and normalize the digit\n",
        "        resized = cv2.resize(digit_roi, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "        normalized = resized.astype('float32') / 255.0\n",
        "\n",
        "        digit_images.append(normalized)\n",
        "\n",
        "    return digit_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jppw5-Bd56H-"
      },
      "source": [
        "### Implementing Multi-Column Table OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okM7Bpyeq8Kc"
      },
      "outputs": [],
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image to be OCR'd\")\n",
        "# ap.add_argument(\"-o\", \"--output\", required=True,\n",
        "# \thelp=\"path to output CSV file\")\n",
        "# ap.add_argument(\"-c\", \"--min-conf\", type=int, default=0,\n",
        "# \thelp=\"minimum confidence value to filter weak text detection\")\n",
        "# ap.add_argument(\"-d\", \"--dist-thresh\", type=float, default=25.0,\n",
        "# \thelp=\"distance threshold cutoff for clustering\")\n",
        "# ap.add_argument(\"-s\", \"--min-size\", type=int, default=2,\n",
        "# \thelp=\"minimum cluster size (i.e., # of entries in column)\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values # michael_jordan_stats.png # atevH.jpg\n",
        "args = {\n",
        "\t\"image\": \"atevH.jpg\",\n",
        "\t\"output\": \"results.csv\",\n",
        "\t\"min_conf\": 0,\n",
        "\t\"dist_thresh\": 25.0,\n",
        "\t\"min_size\": 2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31T4v0udHPRK"
      },
      "outputs": [],
      "source": [
        "#Connecting drive with the collab page\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Setting the location of the file to the drive file containing the input data.\n",
        "os.chdir('/content/drive/My Drive/test_file_final_proj')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i29efyhXpcsW"
      },
      "outputs": [],
      "source": [
        "count =0\n",
        "markTables =[]\n",
        "for filename in os.listdir('/content/drive/My Drive/test_file_final_proj'):\n",
        "    question_tables = []\n",
        "    if filename.endswith(\"jpg\"):\n",
        "        # load the input image and convert it to grayscale\n",
        "        image = cv2.imread(filename);\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # initialize a rectangular kernel that is ~5x wider than it is tall,\n",
        "        # then smooth the image using a 3x3 Gaussian blur and then apply a\n",
        "        # blackhat morpholigical operator to find dark regions on a light\n",
        "        # background\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (51, 11))\n",
        "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "        blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
        "\n",
        "        # compute the Scharr gradient of the blackhat image and scale the\n",
        "        # result into the range [0, 255]\n",
        "        grad = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0, ksize=-1)\n",
        "        grad = np.absolute(grad)\n",
        "        (minVal, maxVal) = (np.min(grad), np.max(grad))\n",
        "        grad = (grad - minVal) / (maxVal - minVal)\n",
        "        grad = (grad * 255).astype(\"uint8\")\n",
        "\n",
        "        # apply a closing operation using the rectangular kernel to close\n",
        "        # gaps in between characters, apply Otsu's thresholding method, and\n",
        "        # finally a dilation operation to enlarge foreground regions\n",
        "        grad = cv2.morphologyEx(grad, cv2.MORPH_CLOSE, kernel)\n",
        "        thresh = cv2.threshold(grad, 0, 255,\n",
        "        cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "        thresh = cv2.dilate(thresh, None, iterations=3)\n",
        "        # plt_imshow(\"Thresh\", thresh)\n",
        "        # find contours in the thresholded image and grab the largest one,\n",
        "        # which we will assume is the stats table\n",
        "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
        "        cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(cnts)\n",
        "\n",
        "        detected_tables = []\n",
        "        for i in range(len(cnts)):\n",
        "            table_cnt = cnts[i]\n",
        "            (x, y, w, h) = cv2.boundingRect(table_cnt)\n",
        "            table = image[y:y + h, x:x + w]\n",
        "\n",
        "            #converting images into binary edge files and saving them to detected_tables\n",
        "            img = cv2.cvtColor(table, cv2.COLOR_BGR2GRAY)\n",
        "            assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
        "            edges = cv2.Canny(img,100,250)\n",
        "            #inverting the processed image\n",
        "            edges = cv2.bitwise_not(edges)\n",
        "\n",
        "            detected_tables.append(edges)\n",
        "\n",
        "            # plt_imshow(\"Table\",table)\n",
        "            if table.size != 0:  # Check if table is not empty\n",
        "                #Detecting the tables that contains the mark data by using pytesseract ocr by detecting the keyword \"Question\"\n",
        "                options = \"--psm 3\"\n",
        "                results = pytesseract.image_to_string(\n",
        "                    cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB),\n",
        "                    config=options\n",
        "                )\n",
        "\n",
        "                if(results.find(\"Question\") != -1):\n",
        "                  question_tables.append(table)\n",
        "                  plt_imshow(\"Table\",edges)\n",
        "                  count+=1\n",
        "\n",
        "        # print(len(question_tables)) #Prints the number of question tables detected in a answer sheet.\n",
        "\n",
        "        mostRedImage = find_most_red_image(question_tables)\n",
        "        if mostRedImage is not None:\n",
        "          plt_imshow(\"Red Image\", mostRedImage)\n",
        "          markTables.append(mostRedImage)\n",
        "\n",
        "\n",
        "        # The below lines contains the code to detect the elements with the most red elements in it but it is not working to find the table that contains the marks.\n",
        "        # mark_table = find_most_red_image(detected_tables)\n",
        "        # cv2_imshow(mark_table)\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcqc_yi459RW"
      },
      "source": [
        "## The below code block contains a number of different approaches to detect text regions in a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCcZ5dM55tjT"
      },
      "outputs": [],
      "source": [
        "#This method uses open cv libraries to try and remove the table in order to find out the text regions.\n",
        "def findTextRegion1(image):\n",
        "    original = image.copy()\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    canny = cv2.Canny(gray, 130, 255, 1)\n",
        "\n",
        "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,2))\n",
        "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,1))\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,3))\n",
        "    erode = cv2.erode(canny, vertical_kernel)\n",
        "    print(\"remove horizontal\")\n",
        "    cv2_imshow(erode)\n",
        "    dilate = cv2.dilate(erode, vertical_kernel, iterations=5)\n",
        "    print('dilate vertical')\n",
        "    cv2_imshow(dilate)\n",
        "    erode = cv2.erode(dilate, horizontal_kernel, iterations=1)\n",
        "    print('remove vertical')\n",
        "    cv2_imshow(erode)\n",
        "    dilate = cv2.dilate(erode, kernel, iterations=4)\n",
        "    print('dilate horizontal')\n",
        "    cv2_imshow(dilate)\n",
        "\n",
        "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "\n",
        "    digit_contours = []\n",
        "    for c in cnts:\n",
        "        area = cv2.contourArea(c)\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        approx = cv2.approxPolyDP(c, 0.01 * peri, True)\n",
        "        x,y,w,h = cv2.boundingRect(approx)\n",
        "        aspect_ratio = w / float(h)\n",
        "\n",
        "        if (aspect_ratio >= 0.4 and aspect_ratio <= 1.3):\n",
        "            if area > 150:\n",
        "                ROI = original[y:y+h, x:x+w]\n",
        "                cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "                digit_contours.append(c)\n",
        "\n",
        "    sorted_digit_contours = contours.sort_contours(digit_contours, method='left-to-right')[0]\n",
        "    contour_number = 0\n",
        "    for c in sorted_digit_contours:\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        ROI = original[y:y+h, x:x+w]\n",
        "        character = cv2.cvtColor(table, cv2.COLOR_BGR2RGB)\n",
        "        assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
        "        cv2.imwrite('ROI_{}.png'.format(contour_number), ROI)\n",
        "        contour_number += 1\n",
        "    print(\"canny\")\n",
        "    cv2_imshow(canny)\n",
        "    print('image')\n",
        "    cv2_imshow(image)\n",
        "\n",
        "#using Img2table to detext text regions from the image.\n",
        "def findTextRegion2(image):\n",
        "      image1 = im.fromarray(image);\n",
        "      image1.save('image.png')\n",
        "      doc = Image('image.png', detect_rotation=False)\n",
        "      assert doc is not None, \"file could not be read, check with os.path.exists()\"\n",
        "      extracted_tables = doc.extract_tables(\n",
        "                                    implicit_rows=False,\n",
        "                                    borderless_tables=False,\n",
        "                                    min_confidence=98)\n",
        "      mark_list = []\n",
        "      for table in extracted_tables:\n",
        "        for id_row, row in enumerate(table.content.values()):\n",
        "          for id_col, cell in enumerate(row):\n",
        "            # if id_row==3 and id_col>0:\n",
        "              x1 = cell.bbox.x1+5\n",
        "              y1 = cell.bbox.y1+5\n",
        "              x2 = cell.bbox.x2-5\n",
        "              y2 = cell.bbox.y2-5\n",
        "              value = cell.value\n",
        "              #print(type(value))\n",
        "              # cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "              cropped_cell = image[y1:y2, x1:x2]\n",
        "              if cropped_cell.shape[0] > 0 and cropped_cell.shape[1] > 0:\n",
        "                print(\"Row ID:\"+str(id_row)+ \" COl ID:\"+str(id_col))\n",
        "                # cv2_imshow(cropped_cell)\n",
        "                # cv2.putText(image, str(id_row) + str(id_col), (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (36,255,12), 2)\n",
        "                hsv = cv2.cvtColor(cropped_cell, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "                # Define the lower and upper boundaries of the red color in the HSV color space\n",
        "                lower_red = np.array([0, 50, 50])\n",
        "                upper_red = np.array([10, 255, 255])\n",
        "\n",
        "                # Convert the image to binary\n",
        "                thresh = cv2.threshold(cropped_cell, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
        "\n",
        "                # Display the image\n",
        "                print('Image')\n",
        "                cv2_imshow(thresh)\n",
        "\n",
        "                #TODO\n",
        "                gray_mark = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
        "                assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
        "                edges = cv2.Canny(gray_mark,100,250)\n",
        "                #edges = cv2.bitwise_not(edges)\n",
        "                cv2_imshow(edges)\n",
        "\n",
        "                mark_list.append(edges)\n",
        "\n",
        "                if thresh.size != 0:  # Check if thresh is not empty\n",
        "                    #Detecting the tables that contains the mark data by using pytesseract ocr by detecting the keyword \"Question\"\n",
        "                    options = \"--psm 10\"\n",
        "                    results = pytesseract.image_to_string(\n",
        "                        cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB),\n",
        "                        config=options\n",
        "                    )\n",
        "                    print('Result:')\n",
        "                    print(results)\n",
        "\n",
        "                    results = pytesseract.image_to_string(\n",
        "                        cv2.cvtColor(gray_mark, cv2.COLOR_GRAY2RGB),\n",
        "                        config=options\n",
        "                    )\n",
        "                    print('Result:')\n",
        "                    print(results)\n",
        "\n",
        "\n",
        "              else:\n",
        "                print(\"empty cell!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuiy1X3K5zmC"
      },
      "outputs": [],
      "source": [
        "for table in markTables:\n",
        "    findTextRegion2(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zAckSQgGaGHY",
        "NFhAzQB3aNMa",
        "wcrOk6pURp50"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}